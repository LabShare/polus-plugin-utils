author:
- Gauhar Bains
- Najib Ishaq
- Madhuri Vihani
- Benjamin Houghton
contact: gauhar.bains@labshare.org
container: polusai/smp-training-plugin:0.5.11
description: Segmentation models training and inference plugin.
entrypoint: '[python3, main.py]'
inputs:
- description: '''active'' or ''inactive'' for whether to run in inference mode.'
  format:
  - inferenceMode
  name: inferenceMode
  required: true
  type: string
- description: Collection containing images on which to run inference.
  format:
  - imagesInferenceDir
  name: imagesInferenceDir
  required: false
  type: path
- description: Filename pattern for images on which to run inference.
  format:
  - inferencePattern
  name: inferencePattern
  required: false
  type: string
- description: 'Path to a model that was previously trained with this plugin. If starting
    fresh, you must instead provide: ''modelName'', ''encoderBase'', ''encoderVariant'',
    ''encoderWeights'', and ''optimizerName''. See the README for available options.'
  format:
  - pretrainedModel
  name: pretrainedModel
  required: false
  type: path
- description: Model architecture to use. Required if starting fresh.
  format:
  - modelName
  name: modelName
  required: false
  type: string
- description: The name of the base encoder to use.
  format:
  - encoderBase
  name: encoderBase
  required: false
  type: string
- description: The name of the specific variant to use.
  format:
  - encoderVariant
  name: encoderVariant
  required: false
  type: string
- description: The name of the pretrained weights to use.
  format:
  - encoderWeights
  name: encoderWeights
  required: false
  type: string
- description: Name of optimization algorithm to use for training the model. Required
    if starting fresh.
  format:
  - optimizerName
  name: optimizerName
  required: false
  type: string
- description: Size of each batch for training. If left unspecified, we use the maximum
    possible based on memory constraints.
  format:
  - batchSize
  name: batchSize
  required: false
  type: number
- description: Collection containing images to use for training.
  format:
  - imagesTrainDir
  name: imagesTrainDir
  required: false
  type: path
- description: Collection containing labels, i.e. the ground-truth, for the training
    images.
  format:
  - labelsTrainDir
  name: labelsTrainDir
  required: false
  type: path
- description: Filename pattern for training images and labels.
  format:
  - trainPattern
  name: trainPattern
  required: false
  type: string
- description: Collection containing images to use for validation.
  format:
  - imagesValidDir
  name: imagesValidDir
  required: false
  type: path
- description: Collection containing labels, i.e. the ground-truth, for the validation
    images.
  format:
  - labelsValidDir
  name: labelsValidDir
  required: false
  type: path
- description: Filename pattern for validation images and labels.
  format:
  - validPattern
  name: validPattern
  required: false
  type: string
- description: Which device to use for the model
  format:
  - device
  name: device
  required: false
  type: string
- description: How often to save model checkpoints
  format:
  - checkpointFrequency
  name: checkpointFrequency
  required: false
  type: number
- description: Name of loss function to use.
  format:
  - lossName
  name: lossName
  required: false
  type: string
- description: Maximum number of epochs for which to continue training the model.
  format:
  - maxEpochs
  name: maxEpochs
  required: false
  type: number
- description: Maximum number of epochs to wait for model to improve.
  format:
  - patience
  name: patience
  required: false
  type: number
- description: Minimum improvement in loss to reset patience.
  format:
  - minDelta
  name: minDelta
  required: false
  type: number
name: polusai/SegmentationModelsTrainingandInference
outputs:
- description: In training mode, this contains the trained model and checkpoints.
    In inference mode, this contains the output labels.
  format:
  - outputDir
  name: outputDir
  required: true
  type: path
repository: https://github.com/PolusAI/polus-plugins/tree/dev/segmentation
specVersion: 1.0.0
title: Segmentation Models Training and Inference
ui:
- description: '''active'' or ''inactive'' for whether to run in inference mode.'
  fields:
  - active
  - inactive
  key: inputs.inferenceMode
  title: inferenceMode
  type: select
- condition: inputs.inferenceMode==active
  description: Collection containing images on which to run inference.
  key: inputs.imagesInferenceDir
  title: imagesInferenceDir
  type: path
- condition: inputs.inferenceMode==active
  description: Filename pattern for images on which to run inference.
  key: inputs.inferencePattern
  title: inferencePattern
  type: text
- description: 'Path to a model that was previously trained with this plugin. If starting
    fresh, you must instead provide: ''modelName'', ''encoderBase'', ''encoderVariant'',
    ''encoderWeights'', and ''optimizerName''. See the README for available options.'
  key: inputs.pretrainedModel
  title: pretrainedModel
  type: path
- description: Model architecture to use. Required if starting fresh.
  fields:
  - Unet
  - UnetPlusPlus
  - MAnet
  - Linknet
  - FPN
  - PSPNet
  - PAN
  - DeepLabV3
  - DeepLabV3Plus
  key: inputs.modelName
  title: modelName
  type: select
- description: The name of the base encoder to use.
  fields:
  - ResNet
  - ResNeXt
  - ResNeSt
  - Res2Ne(X)t
  - RegNet(x/y)
  - GERNet
  - SE-Net
  - SK-ResNe(X)t
  - DenseNet
  - Inception
  - EfficientNet
  - MobileNet
  - DPN
  - VGG
  key: inputs.encoderBase
  title: encoderBase
  type: select
- description: The name of the specific variant to use.
  fields:
  - resnet18
  - resnet34
  - resnet50
  - resnet101
  - resnet152
  - resnext50_32x4d
  - resnext101_32x4d
  - resnext101_32x8d
  - resnext101_32x16d
  - resnext101_32x32d
  - resnext101_32x48d
  - timm-resnest14d
  - timm-resnest26d
  - timm-resnest50d
  - timm-resnest101e
  - timm-resnest200e
  - timm-resnest269e
  - timm-resnest50d_4s2x40d
  - timm-resnest50d_1s4x24d
  - timm-res2net50_26w_4s
  - timm-res2net101_26w_4s
  - timm-res2net50_26w_6s
  - timm-res2net50_26w_8s
  - timm-res2net50_48w_2s
  - timm-res2net50_14w_8s
  - timm-res2next50
  - timm-regnetx_002
  - timm-regnetx_004
  - timm-regnetx_006
  - timm-regnetx_008
  - timm-regnetx_016
  - timm-regnetx_032
  - timm-regnetx_040
  - timm-regnetx_064
  - timm-regnetx_080
  - timm-regnetx_120
  - timm-regnetx_160
  - timm-regnetx_320
  - timm-regnety_002
  - timm-regnety_004
  - timm-regnety_006
  - timm-regnety_008
  - timm-regnety_016
  - timm-regnety_032
  - timm-regnety_040
  - timm-regnety_064
  - timm-regnety_080
  - timm-regnety_120
  - timm-regnety_160
  - timm-regnety_320
  - timm-gernet_s
  - timm-gernet_m
  - timm-gernet_l
  - senet154
  - se_resnet50
  - se_resnet101
  - se_resnet152
  - se_resnext50_32x4d
  - se_resnext101_32x4d
  - timm-skresnet18
  - timm-skresnet34
  - timm-skresnext50_32x4d
  - densenet121
  - densenet169
  - densenet201
  - densenet161
  - inceptionresnetv2
  - inceptionv4
  - xception
  - efficientnet-b0
  - efficientnet-b1
  - efficientnet-b2
  - efficientnet-b3
  - efficientnet-b4
  - efficientnet-b5
  - efficientnet-b6
  - efficientnet-b7
  - timm-efficientnet-b0
  - timm-efficientnet-b1
  - timm-efficientnet-b2
  - timm-efficientnet-b3
  - timm-efficientnet-b4
  - timm-efficientnet-b5
  - timm-efficientnet-b6
  - timm-efficientnet-b7
  - timm-efficientnet-b8
  - timm-efficientnet-l2
  - timm-efficientnet-lite0
  - timm-efficientnet-lite1
  - timm-efficientnet-lite2
  - timm-efficientnet-lite3
  - timm-efficientnet-lite4
  - mobilenet_v2
  - timm-mobilenetv3_large_075
  - timm-mobilenetv3_large_100
  - timm-mobilenetv3_large_minimal_100
  - timm-mobilenetv3_small_075
  - timm-mobilenetv3_small_100
  - timm-mobilenetv3_small_minimal_100
  - dpn68
  - dpn68b
  - dpn92
  - dpn98
  - dpn107
  - dpn131
  - vgg11
  - vgg11_bn
  - vgg13
  - vgg13_bn
  - vgg16
  - vgg16_bn
  - vgg19
  - vgg19_bn
  key: inputs.encoderVariant
  title: encoderVariant
  type: select
- description: The name of the pretrained weights to use.
  fields:
  - advprop
  - imagenet
  - imagenet+5k
  - imagenet+background
  - instagram
  - noisy-student
  - random
  - ssl
  - swsl
  key: inputs.encoderWeights
  title: encoderWeights
  type: select
- condition: inputs.inferenceMode==inactive
  description: Name of optimization algorithm to use for training the model. Required
    if starting fresh.
  fields:
  - Adadelta
  - Adagrad
  - Adam
  - AdamW
  - SparseAdam
  - Adamax
  - ASGD
  - LBFGS
  - RMSprop
  - Rprop
  - SGD
  key: inputs.optimizerName
  title: optimizerName
  type: select
- condition: inputs.inferenceMode==inactive
  description: Size of each batch for training. If left unspecified, we use the maximum
    possible based on memory constraints.
  key: inputs.batchSize
  title: batchSize
  type: number
- condition: inputs.inferenceMode==inactive
  description: Collection containing images to use for training.
  key: inputs.imagesTrainDir
  title: imagesTrainDir
  type: path
- condition: inputs.inferenceMode==inactive
  description: Collection containing labels, i.e. the ground-truth, for the training
    images.
  key: inputs.labelsTrainDir
  title: labelsTrainDir
  type: path
- condition: inputs.inferenceMode==inactive
  description: Filename pattern for training images and labels.
  key: inputs.trainPattern
  title: trainPattern
  type: text
- condition: inputs.inferenceMode==inactive
  description: Collection containing images to use for validation.
  key: inputs.imagesValidDir
  title: imagesValidDir
  type: path
- condition: inputs.inferenceMode==inactive
  description: Collection containing labels, i.e. the ground-truth, for the validation
    images.
  key: inputs.labelsValidDir
  title: labelsValidDir
  type: path
- condition: inputs.inferenceMode==inactive
  description: Filename pattern for validation images and labels.
  key: inputs.validPattern
  title: validPattern
  type: text
- description: Which device to use for the model
  key: inputs.device
  title: device
  type: text
- condition: inputs.inferenceMode==inactive
  description: How often to save model checkpoints
  key: inputs.checkpointFrequency
  title: checkpointFrequency
  type: number
- condition: inputs.inferenceMode==inactive
  description: Name of loss function to use.
  fields:
  - JaccardLoss
  - DiceLoss
  - TverskyLoss
  - FocalLoss
  - LovaszLoss
  - SoftBCEWithLogitsLoss
  - SoftCrossEntropyLoss
  - MCCLoss
  key: inputs.lossName
  title: lossName
  type: select
- condition: inputs.inferenceMode==inactive
  default: 100
  description: Maximum number of epochs for which to continue training the model.
  key: inputs.maxEpochs
  title: maxEpochs
  type: number
- condition: inputs.inferenceMode==inactive
  default: 10
  description: Maximum number of epochs to wait for model to improve.
  key: inputs.patience
  title: patience
  type: number
- condition: inputs.inferenceMode==inactive
  default: 0.0001
  description: Minimum improvement in loss to reset patience.
  key: inputs.minDelta
  title: minDelta
  type: number
version: 0.5.11
